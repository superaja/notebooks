{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13f.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1YEy-jyemdq00hWabag1dPRHnKIRB28Aq",
      "authorship_tag": "ABX9TyPvSD1L612vuhbQL6RI01rz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5512631d80b9428e9e866fabea1392e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_18c4a76bef08406f843706e559ed38bc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c901f411ef00493191a7b1e085989bb8",
              "IPY_MODEL_036d7a7e6c904cf08e0e6cceb8831879"
            ]
          }
        },
        "18c4a76bef08406f843706e559ed38bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c901f411ef00493191a7b1e085989bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_48b53a14577b4905816ac7df5196f4e1",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 828,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 828,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a39f5231cbcc4248a410ef5ccd686927"
          }
        },
        "036d7a7e6c904cf08e0e6cceb8831879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c6a0b0096217468d8d0b3338d488dcb9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 828/828 [03:01&lt;00:00,  4.56it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_33acd37f9e2e47f489dc195831d38e1b"
          }
        },
        "48b53a14577b4905816ac7df5196f4e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a39f5231cbcc4248a410ef5ccd686927": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6a0b0096217468d8d0b3338d488dcb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "33acd37f9e2e47f489dc195831d38e1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "222633e10ddc4e6fa0a4b8c43d8d54e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_53e1d3908b2c4428bdef328aeafe9888",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_832b881ad5d3449e8579448f22f70566",
              "IPY_MODEL_e48dc694c3a2458abb2b9d7739228362"
            ]
          }
        },
        "53e1d3908b2c4428bdef328aeafe9888": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "832b881ad5d3449e8579448f22f70566": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f5426d7f81f5442e826b87034ec4f5f8",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1624,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1624,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_468c6f25445e49b28f0e2fc31f44a2a7"
          }
        },
        "e48dc694c3a2458abb2b9d7739228362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4ea0319fc075470cb33e7e5bde490806",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1624/1624 [20:33&lt;00:00,  1.32it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8d674cbc01424bd0ae50eac1b675080f"
          }
        },
        "f5426d7f81f5442e826b87034ec4f5f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "468c6f25445e49b28f0e2fc31f44a2a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4ea0319fc075470cb33e7e5bde490806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8d674cbc01424bd0ae50eac1b675080f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/superaja/notebooks/blob/master/13f.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llZChhqKdcx_"
      },
      "source": [
        "# Quarterly 13F\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import uuid\n",
        "import datetime\n",
        "from os import path\n",
        "import os\n",
        "import json\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from pymongo import MongoClient\n",
        "import uuid\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from gspread_dataframe import set_with_dataframe"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp3e7bIqTbpt",
        "outputId": "46b7620d-d448-46bf-9af9-bbd6757bfb95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# Load Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sGkRQcabWf-"
      },
      "source": [
        "# load gsheet auth\n",
        "auth.authenticate_user()\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXe03PGsGzRZ"
      },
      "source": [
        "# helper functions\n",
        "def accession_number_parser(an):\n",
        "  split_list = an.split(\"-\")\n",
        "  sub_url1 = split_list[0].lstrip(\"0\")\n",
        "  sub_url2 = str(split_list[0]+ split_list[1] + split_list[2])\n",
        "  return sub_url1, sub_url2\n",
        "\n",
        "def check_empty(file):\n",
        "  path_data = '/content/drive/My Drive/Analytics/13fdata/Q420data/'\n",
        "  df = pd.read_csv(path_data+file)\n",
        "  r, c = df.shape\n",
        "  if r == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "def accession_number_check(an):\n",
        "  path_downloaded = '/content/drive/My Drive/Analytics/13fdata/'\n",
        "  download_status_df = pd.read_csv(path_downloaded+\"download_status.csv\")\n",
        "  acc_number = list(download_status_df['Accession Number'].unique())\n",
        "  if str(an) in acc_number:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "def reset_rank():\n",
        "  # reset rank in meta file\n",
        "  rank_reset_df = pd.read_csv('/content/drive/My Drive/Analytics/13fdata/download_status.csv')\n",
        "  rank_reset_df['Qrank'] = rank_reset_df.groupby('Filer')['Time'].rank(ascending=True, method=\"first\")\n",
        "  if path.exists('/content/drive/My Drive/Analytics/13fdata/' + \"download_status.csv\"):\n",
        "    rank_reset_df.to_csv('/content/drive/My Drive/Analytics/13fdata/' + \"download_status.csv\", mode='a', header=False, index=False)\n",
        "  else: \n",
        "    rank_reset_df.to_csv('/content/drive/My Drive/Analytics/13fdata/' + \"download_status.csv\", index=False)\n",
        "\n",
        "#cusip lookup\n",
        "def cusip_lookup(cusip):\n",
        "  symList = []\n",
        "  #path_to_lkp = '/content/drive/My Drive/Analytics/13fdata/CUSIP_LKP.csv'\n",
        "  newCUSIP = []\n",
        "  url = \"https://quotes.fidelity.com/mmnet/SymLookup.phtml?reqforlookup=REQUESTFORLOOKUP&productid=mmnet&isLoggedIn=mmnet&rows=50&for=stock&by=cusip&criteria=\"+cusip+\"&submit=Search\"\n",
        "  resp = requests.get(url=url)\n",
        "  soup = BeautifulSoup(resp.content, 'html.parser' )\n",
        "  #print (soup.find_all('font', {\"class\": \"smallfont\"})[0].text)\n",
        "  sym_list = soup.find_all('a')\n",
        "  for s in sym_list:\n",
        "    x = re.search(\"QUOTE_TYPE\", s.get(\"href\"))\n",
        "    if x:\n",
        "      symList.append(s.text)\n",
        "    else:\n",
        "      pass\n",
        "  if len(symList) > 0:\n",
        "    cusip_mdb(cusip, symList[0], \"\")\n",
        "    '''\n",
        "    newCUSIP.append({\n",
        "        \"SYMBOL\": symList[0],\n",
        "        \"CUSIP\": str(cusip)\n",
        "    })\n",
        "    cusip_df = pd.DataFrame(newCUSIP, columns=[\"SYMBOL\", \"CUSIP\"]) # Replace\n",
        "    cusip_df.to_csv(path_to_lkp, mode='a', header=False, index=False) # Replace\n",
        "    #return symList[0]\n",
        "    '''\n",
        "  else:\n",
        "    cusip_mdb(cusip, \"Not Found\", \"\")\n",
        "    '''\n",
        "    newCUSIP.append({\n",
        "        \"SYMBOL\": \"Not Found\",\n",
        "        \"CUSIP\": str(cusip)\n",
        "    })\n",
        "    cusip_df = pd.DataFrame(newCUSIP, columns=[\"SYMBOL\", \"CUSIP\"]) # Replace\n",
        "    print (cusip_df)\n",
        "    cusip_df.to_csv(path_to_lkp, mode='a', header=False, index=False) # Replace\n",
        "    #return \"Not Found\"\n",
        "    '''\n",
        "\n",
        "# Function to remove errored filers\n",
        "\n",
        "def ignoredFilers():\n",
        "  meta_df = pd.read_csv('/content/drive/My Drive/Analytics/13fdata/download_status.csv')\n",
        "  filers = list(meta_df[meta_df[\"Process Status\"].isin([\"DQ Check\", \"Error in QoQ\", \"Empty File\"])][\"Filer\"].unique())\n",
        "  meta_df.loc[meta_df[\"Filer\"].isin(filers),\"Process Status\"] = \"Not Included\" # Updates bad filers\n",
        "  print(\"Total Filers Ignored: \", len(filers))\n",
        "  meta_df.to_csv('/content/drive/My Drive/Analytics/13fdata/download_status.csv', index=False)\n",
        "\n",
        "\n",
        "def dqCheck(files):\n",
        "  # check empty files = non=parallel\n",
        "  try:\n",
        "    all_df = []\n",
        "    for f in tqdm(files):\n",
        "      fileCheck = check_empty(f)\n",
        "      if fileCheck != 1:\n",
        "        #print(f, \" is Empty\")\n",
        "        all_df.append(f)\n",
        "      else: \n",
        "        pass\n",
        "    return all_df\n",
        "  except Exception as e:\n",
        "    se = str(e)\n",
        "    return se\n",
        "\n",
        "def dqCheckP(f):\n",
        "  # check empty files = Parallel\n",
        "  try:\n",
        "    fileCheck = check_empty(f)\n",
        "    if fileCheck != 1:\n",
        "      print(f, \" is Empty\")\n",
        "      return f\n",
        "    else: \n",
        "      pass\n",
        "  except Exception as e:\n",
        "    se = str(e)\n",
        "    return se\n",
        "\n",
        "# size\n",
        "\n",
        "def fileSize(f):\n",
        "  b = os.path.getsize(\"/content/drive/My Drive/Analytics/13fdata/Q420data/\"+f)\n",
        "  return b/1e3\n",
        "\n",
        "# mongo helper\n",
        "mongo_path = 'mongodb://osadmin:Subzero!4@ds145304.mlab.com:45304/os?retryWrites=false'\n",
        "\n",
        "def get_mongo_client():\n",
        "  client = MongoClient(mongo_path)\n",
        "  return client.os\n",
        "\n",
        "def get_header_info(accession_number):\n",
        "  base_url = \"https://www.sec.gov/Archives/edgar/data/\"\n",
        "  sub_url1, sub_url2 = accession_number_parser(str(accession_number))\n",
        "  final_url = base_url+ sub_url1 + '/' + sub_url2 + '/' + accession_number + '-index.htm'\n",
        "  r = requests.get(final_url)\n",
        "  # Parse\n",
        "  s = BeautifulSoup(r.content, 'html.parser')\n",
        "  links = s.find_all('a')\n",
        "  sub = \".txt\"\n",
        "  for l in links:\n",
        "    if sub in l.text:\n",
        "      summary_xml = 'https://www.sec.gov'+l.get('href')\n",
        "\n",
        "  r = requests.get(summary_xml)\n",
        "  s = BeautifulSoup(r.content, 'html.parser')\n",
        "  market_value_reported = s.find('tablevaluetotal')\n",
        "  total_rows = s.find('tableentrytotal')\n",
        "  if market_value_reported:\n",
        "    market_value_reported = market_value_reported.text\n",
        "  else: \n",
        "    market_value_reported = 0\n",
        "  if total_rows:\n",
        "    total_rows = total_rows.text\n",
        "  else: \n",
        "    total_rows = 0\n",
        "  \n",
        "  return int(market_value_reported)*1000, int(total_rows)\n",
        "\n",
        "db = get_mongo_client()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awl4aArKi_PL",
        "outputId": "b4a5b701-9ab2-4ab2-d6d4-c9785a02c288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "get_header_info('0000950123-20-010233')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9407796000, 607)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sJDr5MV9plD"
      },
      "source": [
        "# Works but not used\n",
        "def cusipLkp(cusip):\n",
        "  cusipType = ['stock', 'index', 'fund']\n",
        "  symbol = []\n",
        "  base_url = \"https://quotes.fidelity.com/mmnet/SymLookup.phtml?reqforlookup=REQUESTFORLOOKUP&productid=mmnet&isLoggedIn=mmnet&rows=50&for=\"\n",
        "  for ct in cusipType:\n",
        "    symList = []\n",
        "    url = base_url + ct + \"&by=cusip&criteria=\"+cusip+\"&submit=Search\"\n",
        "    resp = requests.get(url=url)\n",
        "    soup = BeautifulSoup(resp.content, 'html.parser' )\n",
        "    sym_list = soup.find_all('a')\n",
        "    for s in sym_list:\n",
        "      x = re.search(\"QUOTE_TYPE\", s.get(\"href\"))\n",
        "      if x:\n",
        "        symList.append(s.text)\n",
        "    \n",
        "    if len(symList) > 0:\n",
        "      symbol.append(symList[0])\n",
        "\n",
        "  if len(symbol) > 0:\n",
        "    pass\n",
        "  else:\n",
        "    symbol = [\"Not Available\"]\n",
        "\n",
        "  return symbol[0]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HVDrELIL_T_"
      },
      "source": [
        "# Data Extraction\n",
        "\n",
        "'''\n",
        "1. Identify empty / wrong infoTable - log it as empty table in meta file\n",
        "2. Dynamically change Quarter Location for downloaded files\n",
        "3. Archive older files\n",
        "4. Log meta into a database and not file\n",
        "5. \n",
        "'''\n",
        "\n",
        "def get13FData(cik_dict, q):\n",
        "  db = get_mongo_client()\n",
        "  folder = q+\"data\"\n",
        "\n",
        "  f = open('/content/drive/My Drive/Analytics/13fdata/log.txt', 'a+')\n",
        "\n",
        "  # define end point\n",
        "  endpoint = \"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
        "\n",
        "  for cik, name in cik_dict.items():\n",
        "    print (\"Extracting for \" + name)\n",
        "    param_dict = {\"action\": \"getcompany\",\n",
        "                  \"CIK\": cik,\n",
        "                  \"type\":\"13F\",\n",
        "                  \"dateb\": \"\",\n",
        "                  \"owner\": \"\",\n",
        "                  \"include\": \"\",\n",
        "                  \"count\": \"8\", # last 2 years\n",
        "                  \"output\": \"atom\"}\n",
        "    response = requests.get(url=endpoint, params = param_dict)\n",
        "    soup = BeautifulSoup(response.content, 'lxml')\n",
        "    entries = soup.find_all('entry')\n",
        "\n",
        "    base_url = \"https://www.sec.gov/Archives/edgar/data/\" # for info_table\n",
        "\n",
        "    \n",
        "\n",
        "    for entry in entries:\n",
        "      try:\n",
        "        download_data = []\n",
        "        accession_number = entry.find('accession-number').text\n",
        "        #print(accession_number)\n",
        "        check_an = accession_number_check_mdb(accession_number) # check if file already downloaded\n",
        "        if check_an == 1:\n",
        "          print(accession_number, check_an)\n",
        "          sub_url1, sub_url2 = accession_number_parser(str(accession_number))\n",
        "          final_url = base_url+ sub_url1 + '/' + sub_url2 + '/' + accession_number + '-index.htm'\n",
        "          r = requests.get(final_url)\n",
        "          print(r.status_code)\n",
        "          # Parse\n",
        "          s = BeautifulSoup(r.content, 'html.parser')\n",
        "          links = s.find_all('a')\n",
        "          sub = \".xml\"\n",
        "          for l in links:\n",
        "            if sub in l.text and \"primary\" not in l.text:\n",
        "              info_table_xml = 'https://www.sec.gov'+l.get('href')\n",
        "              print(l.text, info_table_xml)\n",
        "              log_str = l.text + \", \" + info_table_xml\n",
        "              f.write(log_str)\n",
        "          \n",
        "          # filing metadata\n",
        "          event = s.findAll(\"div\", {\"class\":\"infoHead\"})\n",
        "          date = s.findAll(\"div\", {\"class\": \"info\"})\n",
        "          final_event = [e.text for e in event]\n",
        "          final_date = [d.text for d in date]\n",
        "          file_meta = dict(zip(final_event, final_date))\n",
        "\n",
        "        \n",
        "          # extract the info table\n",
        "          if info_table_xml:\n",
        "            resp = requests.get(info_table_xml)\n",
        "            soup = BeautifulSoup(resp.content, 'xml')\n",
        "            row = soup.find_all('infoTable')\n",
        "            \n",
        "            each_row = []\n",
        "\n",
        "            df_cols = [\"Filer\", \"Stock\", \"cusip\", \"Title of Class\", \"Market Value\", \n",
        "                      \"Shares Held\", \"Entity Type\", \"Put Call\", \"Investment Discretion\", \n",
        "                      \"Sole\", \"Shared\", \"Source Date\", \"Reporting Period\"]\n",
        "            \n",
        "            download_status_cols = [\"Accession Number\", \"Filer\", \"Source Date\", \"File Name\", \n",
        "                                    \"Download Status\", \"Download Date\", \"Process Status\",\n",
        "                                    \"Time\", \"Source Quarter\", \"Qrank\"]\n",
        "            for r in row:\n",
        "\n",
        "              # Handle missing fields\n",
        "              fields = [r.nameOfIssuer,r.cusip,r.titleOfClass,r.value, r.shrsOrPrnAmt.sshPrnamt,\n",
        "                        r.shrsOrPrnAmt.sshPrnamtType,r.putCall, r.investmentDiscretion,\n",
        "                        r.votingAuthority, r.votingAuthority.Sole,r.votingAuthority.Shared]\n",
        "              row_dict = {}\n",
        "              field_dict = {\"Stock\": r.nameOfIssuer, \n",
        "                              \"cusip\": r.cusip,\n",
        "                              \"Title of Class\": r.titleOfClass, \n",
        "                              \"Market Value\": r.value,\n",
        "                              \"Shares Held\":r.shrsOrPrnAmt.sshPrnamt,\n",
        "                              \"Entity Type\":r.shrsOrPrnAmt.sshPrnamtType, \n",
        "                              \"Put Call\": r.putCall,\n",
        "                              \"Investment Discretion\": r.investmentDiscretion,\n",
        "                              \"Voting Authority\": r.votingAuthority,\n",
        "                              \"Sole\": r.votingAuthority.Sole,\n",
        "                              \"Shared\": r.votingAuthority.Shared, \n",
        "                              }\n",
        "              for col, field in field_dict.items(): \n",
        "                if field:\n",
        "                  row_dict.update({col: field.string})\n",
        "                else: \n",
        "                  row_dict.update({col:\"N/A\"})\n",
        "              each_row.append(row_dict)\n",
        "              \n",
        "            out_df = pd.DataFrame(each_row, columns = df_cols)\n",
        "\n",
        "            # Additional Parameters\n",
        "            out_df['Filer'] = name\n",
        "            if file_meta['Filing Date']:\n",
        "              out_df['Source Date'] = file_meta['Filing Date']\n",
        "            else: \n",
        "              out_df['Source Date'] = \"Not Available\"\n",
        "            out_df['Reporting Period'] = file_meta['Period of Report']\n",
        "\n",
        "            file_name = name + \"_\"+ file_meta['Filing Date']\n",
        "\n",
        "            out_df.to_csv('/content/drive/My Drive/Analytics/13fdata/'+folder+'/'+ file_name + \"_\" + accession_number +\".csv\", index=False)\n",
        "\n",
        "            # create download meta\n",
        "\n",
        "            download_data.append({\"Accession Number\": accession_number, \n",
        "                              \"Filer\": name, \n",
        "                              \"Source Date\": file_meta['Period of Report'],\n",
        "                              \"File Name\": file_name + \"_\" + accession_number +\".csv\",\n",
        "                              \"Download Status\": \"Complete\" , \n",
        "                              \"Download Date\": datetime.datetime.now(),\n",
        "                              \"Process Status\": \"Downloaded\",\n",
        "                              \"Time\": \"\", \n",
        "                              \"Source Quarter\": \"\",\n",
        "                              \"Qrank\": \"\"})\n",
        "            \n",
        "            \n",
        "            print(\"completed file download for \" + accession_number)\n",
        "            f.write(\"completed file download for \" + accession_number)\n",
        "          \n",
        "            download_status_df = pd.DataFrame(download_data, columns = download_status_cols)\n",
        "\n",
        "            # Generate the ranking for the files. \n",
        "            download_status_df['Time'] = pd.to_datetime(download_status_df['Source Date'])\n",
        "            download_status_df['Source Quarter'] = pd.PeriodIndex(download_status_df['Time'], freq='Q').astype(str)\n",
        "            download_status_df['Qrank'] = download_status_df.groupby('Filer')['Time'].rank(ascending=True, method=\"first\")\n",
        "\n",
        "            download_status_df_dict = download_status_df.to_dict(\"records\")\n",
        "            # MONGO DB INSERT HERE - download_status_df insert\n",
        "            db.os13fmeta.insert_many(download_status_df_dict)\n",
        "            '''\n",
        "            if path.exists('/content/drive/My Drive/Analytics/13fdata/' + \"download_status.csv\"):\n",
        "              download_status_df.to_csv('/content/drive/My Drive/Analytics/13fdata/' + \"download_status.csv\", mode='a', header=False, index=False)\n",
        "            else: \n",
        "              download_status_df.to_csv('/content/drive/My Drive/Analytics/13fdata/' + \"download_status.csv\", index=False)\n",
        "          else: \n",
        "            pass # skip the download process'''\n",
        "        else:\n",
        "          print(\"Info table not found or data already downloaded, Skipping\")\n",
        "      except Exception as e:\n",
        "        se = str(e)\n",
        "        print(\"ERROR for Accession Number: \" + accession_number + \"Error:\" + se)\n",
        "        f.write(\"ERROR for Accession Number: \" + accession_number + \"Error: \" + se)\n",
        "        \n",
        "        pass\n",
        "  f.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWH6GyEZkzhc"
      },
      "source": [
        "cik_dict =  {'0001009207': 'D.E. Shaw & Company',\n",
        "               '0001029160': 'Soros Fund Management',\n",
        "               '0001037389': 'Renaissance Technologies',\n",
        "               '0001167483': 'Tiger Global Management',\n",
        "               '0001167557': 'AQR',\n",
        "               '0001350694': 'Bridgewater Associates',\n",
        "               '0001387322': 'Whale Rock Capital Management',\n",
        "               '0001410833': 'Night Owl Capital Management',\n",
        "               '0001423053': 'Citadel Advisors',\n",
        "               '0001537530': 'Scge Management',\n",
        "               '0001553936': 'Tybourne Capital Management',\n",
        "               '0001577133': 'Greenwoods Asset Management',\n",
        "               '0001649339': 'Scion Asset Management',\n",
        "               '0001510281': 'Saba Capital Management',\n",
        "               '0001061165': 'Lone Pine Capital',\n",
        "               '0001061768': 'The Baupost Group',\n",
        "               '0001103804': 'Viking Global Investors',\n",
        "               #'0001048445': 'Elliott Management Corp',\n",
        "               '0001315765': 'Cedar Rock Capital Limited',\n",
        "               '0001656456': 'Appaloosa LP',\n",
        "               '0001273087': 'Millennium Management LLC',\n",
        "               '0000909661': 'Farallon Capital Management',\n",
        "               '0001627436': 'Sylebra HK',\n",
        "               '0001476179':'Firsthand Capital Management',\n",
        "               '0001389933': 'DAFNA Capital Management',\n",
        "               '0001104329': 'CrossLink Capital Inc',\n",
        "               '0001671657': 'Dorsey Asset Management',\n",
        "               '0001599731': 'Atika Capital Management',\n",
        "               '0001067983': 'Berkshire Hathaway'\n",
        "\n",
        "  }\n",
        "\n",
        "############################################################\n",
        "\n",
        "  # Historical Download - RUN THIS CODE ONLY ONE TIME \n",
        "  cik_df = pd.read_csv('/content/drive/My Drive/Analytics/13fdata/cik13ffilers.csv')\n",
        "  cik_df['CIK'] = cik_df['CIK'].astype(str).str.zfill(10)\n",
        "  x = cik_df[['CIK', 'COMPANY']].to_json(orient='records')\n",
        "  parsed = json.loads(x)\n",
        "  cik_dict = {}\n",
        "  for p in parsed:\n",
        "    cik_dict.update({p['CIK']:p['COMPANY']})\n",
        "\n",
        "#############################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXhvN7rF7NRw"
      },
      "source": [
        "def DailyPull():\n",
        "  import datetime\n",
        "  import requests\n",
        "  import pandas as pd\n",
        "  import json\n",
        "\n",
        "  # determine yesterday date  \n",
        "  today = datetime.date.today()\n",
        "  yesterday = today - datetime.timedelta(days=2)\n",
        "  print(yesterday)\n",
        "  year = yesterday.strftime('%Y')\n",
        "  month = yesterday.strftime('%m')\n",
        "  day = yesterday.strftime('%d')\n",
        "  date = year + '-' + month + '-' + day\n",
        "\n",
        "  # Daily pull\n",
        "  url= 'https://www.sec.gov/Archives/edgar/full-index/2020/QTR4/master.idx'\n",
        "  lines = requests.get(url).content.decode(\"utf-8\", \"ignore\").splitlines()\n",
        "  records = [tuple(line.split('|')) for line in lines[11:]]\n",
        "  filers_df = pd.DataFrame(records, columns=[\"CIK\", \"COMPANY\", \"FORM\", \"DATE\", \"URL\"])\n",
        "  filers_13f_df = filers_df[filers_df[\"FORM\"]== \"13F-HR\"]\n",
        "  filers_13f_df['CIK'] = filers_13f_df['CIK'].astype(str).str.zfill(10)\n",
        "  to_be_checked_filers_df = filers_13f_df[filers_13f_df[\"DATE\"] > date]\n",
        "\n",
        "  # convert to dict\n",
        "  filers_json = to_be_checked_filers_df[['CIK', 'COMPANY']].to_json(orient='records')\n",
        "  parsed = json.loads(filers_json)\n",
        "  cik_dict = {}\n",
        "  for p in parsed:\n",
        "    p['COMPANY'] = p['COMPANY'].replace(\"/\", \"\")\n",
        "    cik_dict.update({p['CIK']:p['COMPANY']})\n",
        "\n",
        "  print(cik_dict)\n",
        "  # call data-downloader\n",
        "  get13FData(cik_dict, \"Q420\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNjMegrlwFWA"
      },
      "source": [
        "def DataPipeline():\n",
        "\n",
        "  # download data\n",
        "  # postprocess # 1 - tier 1 transformations\n",
        "  # postprocess # 2 - tier 2 transformations\n",
        "\n",
        "  # define contants\n",
        "\n",
        "  #path_downloaded = '/content/drive/My Drive/Analytics/13fdata/data/'\n",
        "  path_downloaded = '/content/drive/My Drive/Analytics/13fdata/'\n",
        "\n",
        "  # get meta file\n",
        "  meta_df = pd.read_csv(path_downloaded +'download_status.csv')\n",
        "  #files = list(meta_df[(meta_df['Process Status'] == \"Downloaded\")]['File Name'])\n",
        "  files = fileNameExtract()\n",
        "  \n",
        "  # postprocess # 1\n",
        "  #Parallel(n_jobs=4, backend='multiprocessing')(delayed(postProcess)(f) for f in files)\n",
        "  \n",
        "  for f in files:\n",
        "    result = postProcess(f, \"Q420\")\n",
        "  '''\n",
        "  # postprocess # 2\n",
        "  # Reset Ranks\n",
        "  meta_df['Time'] = pd.to_datetime(meta_df['Source Date'])\n",
        "  meta_df['Qrank'] = meta_df.groupby('Filer')['Time'].rank(ascending=True, method=\"first\")\n",
        "  # Update the csv\n",
        "  #meta_df.to_csv('/content/drive/My Drive/Analytics/13fdata/data/download_status.csv', index=False)\n",
        "  meta_df.to_csv('/content/drive/My Drive/Analytics/13fdata/download_status.csv', index=False)\n",
        "  \n",
        "  files_to_be_processed = meta_df[meta_df[\"Process Status\"] == \"Base Processed\"][[\"Filer\", \"File Name\", \"Qrank\"]]\n",
        "  for f in tqdm(list(files_to_be_processed[\"Filer\"].unique())):\n",
        "    chunk = files_to_be_processed[files_to_be_processed[\"Filer\"] == f][[\"File Name\", \"Qrank\"]]\n",
        "    print (chunk[\"Qrank\"].max())\n",
        "    for i in range(int(chunk[\"Qrank\"].min()), int(chunk[\"Qrank\"].max()+1)):\n",
        "      if i == 1:\n",
        "        print(chunk[chunk[\"Qrank\"]==i][\"File Name\"].iloc[0])\n",
        "        first_file_df = initialFile(chunk[chunk[\"Qrank\"]==i][\"File Name\"].iloc[0])\n",
        "      elif chunk[\"Qrank\"].min() == chunk[\"Qrank\"].max():\n",
        "        latest_file_chunk = meta_df[(meta_df['Filer']==f) & (meta_df['Process Status'] == 'Latest File')][['File Name', 'Qrank']]\n",
        "        previous_file = latest_file_chunk[\"File Name\"].iloc[0]\n",
        "        current_file =  chunk[chunk[\"Qrank\"]==i][\"File Name\"].iloc[0]\n",
        "        #print(\"Previous File: \", previous_file)\n",
        "        #print(\"Current File: \", current_file)\n",
        "        processed_df = qoqChange(previous_file, current_file)\n",
        "      else:\n",
        "        print(chunk[chunk[\"Qrank\"]==i][\"File Name\"] + \" is the {0} file\".format(i))\n",
        "        #latest_file_chunk = meta_df[(meta_df['Filer']==f) & (meta_df['Process Status'] == 'Latest File')][['File Name', 'Qrank']]\n",
        "        #previous_file = latest_file_chunk[\"File Name\"].iloc[0]\n",
        "        previous_file = chunk[chunk[\"Qrank\"]==i-1][\"File Name\"].iloc[0]\n",
        "        current_file =  chunk[chunk[\"Qrank\"]==i][\"File Name\"].iloc[0]\n",
        "        processed_df = qoqChange(previous_file, current_file)\n",
        "        #print(\"Previous File: \", previous_file)\n",
        "        #print(\"Current File: \", current_file)\n",
        "\n",
        "        '''"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J2zIgO1wVac"
      },
      "source": [
        "def postProcess(fileName, q):\n",
        "  folder = q+\"data\"\n",
        "  print (\"Processing: \" + fileName)\n",
        "  # get cusip from local file else run cusip_lookup\n",
        "  p = '/content/drive/My Drive/Analytics/13fdata/'\n",
        "  #p1 = '/content/drive/My Drive/Analytics/13fdata/'\n",
        "  #df = pd.read_csv(p+fileName) # read fileName\n",
        "  #df = pd.read_csv(p1+\"newdata/\"+fileName) # Read filename for newdata\n",
        "  df = pd.read_csv(p+folder+'/' + fileName)\n",
        "  download_status_df = pd.read_csv(p+\"download_status.csv\")\n",
        "  try:\n",
        "    # Calculate metrics\n",
        "    df[\"Total Market Value\"] = df[\"Market Value\"]*1000\n",
        "    df[\"Average Price\"] = round(df[\"Total Market Value\"] / df[\"Shares Held\"], 2)\n",
        "    df[\"Reporting Period\"] = pd.to_datetime(df[\"Reporting Period\"])\n",
        "    df[\"Source Date\"] = pd.to_datetime(df[\"Source Date\"])\n",
        "    df[\"Reporting Quarter\"] = pd.PeriodIndex(df['Reporting Period'], freq='Q')\n",
        "    df[\"Source Quarter\"] = pd.PeriodIndex(df['Source Date'], freq='Q')\n",
        "    Total_Market_Value = df[\"Total Market Value\"].sum()\n",
        "    df[\"Percent of Portfolio\"] = round((df[\"Total Market Value\"] / Total_Market_Value)*100, 2)\n",
        "    df[\"Rank\"] = df[\"Percent of Portfolio\"].rank(method=\"max\", ascending=False)\n",
        "\n",
        "\n",
        "    # Column Rename and change dtypes to str\n",
        "    df.rename(columns = {'cusip': 'CUSIP', 'Stock': 'Name'}, inplace=True)\n",
        "    df['CUSIP'] = df['CUSIP'].astype('str')\n",
        "    \n",
        "    #cusip_lkp_df = pd.read_csv(p+\"CUSIP_LKP.csv\", usecols=[\"SYMBOL\", \"CUSIP\"])\n",
        "    cusip_md = getcusip_mdb()\n",
        "    cusip_lkp_df = pd.DataFrame(cusip_md)\n",
        "    cusip_lkp_df = cusip_lkp_df[[\"SYMBOL\", \"CUSIP\"]]\n",
        "    cusip_lkp_df['CUSIP'] = cusip_lkp_df['CUSIP'].astype('str')\n",
        "\n",
        "    # Lookup CUSIP\n",
        "\n",
        "    determine_cusip = list(df[\"CUSIP\"]) # extract cusip to be lookedup\n",
        "    # Find the ones that are available in master cusip lookup\n",
        "    available_cusip = cusip_lkp_df[cusip_lkp_df['CUSIP'].isin(determine_cusip)]\n",
        "    available_cusip_list = list(available_cusip[\"CUSIP\"])\n",
        "\n",
        "    # If the resulting DF is not empty - i.e. Then find the ones missing \n",
        "    if available_cusip.shape[0] > 0:\n",
        "      cusip_to_find = [c for c in determine_cusip if c not in available_cusip_list]\n",
        "      cusip_to_find = list(np.unique(cusip_to_find))\n",
        "      for cf in cusip_to_find:\n",
        "        print(\"calling external for: \" + cf)\n",
        "        cusip_lookup(cf) # find the missing ones and insert it in the master\n",
        "    else:\n",
        "      pass \n",
        "\n",
        "    # Reload refreshed cusip_lkp\n",
        "    #ref_cusip_lkp_df = pd.read_csv(p+\"CUSIP_LKP.csv\", usecols=[\"SYMBOL\", \"CUSIP\"])\n",
        "    ref_cusip_md = getcusip_mdb()\n",
        "    ref_cusip_lkp_df = pd.DataFrame(ref_cusip_md)\n",
        "    ref_cusip_lkp_df = ref_cusip_lkp_df[[\"SYMBOL\", \"CUSIP\"]]\n",
        "    result = pd.merge(df, ref_cusip_lkp_df, on=\"CUSIP\", how=\"inner\" )\n",
        "\n",
        "    # write the results in postprocess\n",
        "    result.to_csv(p+\"post1data/\"+fileName, index=False)\n",
        "\n",
        "    # Update process status\n",
        "    #download_status_df.loc[download_status_df[\"File Name\"] == fileName, \"Process Status\"] = \"Base Processed\"\n",
        "    #download_status_df.to_csv(p+'download_status.csv', index=False)\n",
        "    updateFileStatus(fileName, \"Base Processed\")\n",
        "    print (\"Completed Processing: \" + fileName)\n",
        "\n",
        "    return result\n",
        "  except Exception as e:\n",
        "    se = str(e)\n",
        "    # Update process status\n",
        "    #download_status_df.loc[download_status_df[\"File Name\"] == fileName, \"Process Status\"] = \"Error in Processing\"\n",
        "    updateFileStatus(fileName, \"Error in Processing\")\n",
        "    #download_status_df.to_csv(p+'download_status.csv', index=False)\n",
        "    print(\"ERROR in processing file: \" + fileName + \"Error:\" + se)\n",
        "    return \"Error\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMWlG-PUUZ_v"
      },
      "source": [
        "def qoqChange(fileName_n, fileName_n1):\n",
        "  p1 = '/content/drive/My Drive/Analytics/13fdata/finaldata/'\n",
        "  p ='/content/drive/My Drive/Analytics/13fdata/Q420finaldata/' # change to final data\n",
        "  p2 = '/content/drive/My Drive/Analytics/13fdata/post1data/'\n",
        "\n",
        "  \n",
        "  #read meta file\n",
        "  #download_status_df = pd.read_csv(path_downloaded+\"download_status.csv\")\n",
        "  try:\n",
        "    #file1EmptyCheck = check_empty(fileName_n) #78\n",
        "    #file2EmptyCheck = check_empty(fileName_n1) #79\n",
        "    file1EmptyCheck = 1\n",
        "    file2EmptyCheck = 1\n",
        "    if file1EmptyCheck == 1 & file2EmptyCheck == 1: # check if file is empty\n",
        "      \n",
        "      df_n = pd.read_csv(p1+fileName_n)\n",
        "      df_n1 = pd.read_csv(p2+fileName_n1)\n",
        "\n",
        "      # ensure changes to CUSIP for merge\n",
        "      df_n['CUSIP'] = df_n['CUSIP'].astype('str')\n",
        "      df_n1['CUSIP'] = df_n1['CUSIP'].astype('str')\n",
        "\n",
        "      # Calculate Change in Shares, Calculate Change Status\n",
        "      dfn1 = df_n1.groupby(['Filer','CUSIP', 'SYMBOL', 'Name', 'Entity Type', 'Reporting Period', 'Reporting Quarter', 'Source Date'], as_index=False)[['Shares Held', 'Total Market Value']].sum()\n",
        "      dfn = df_n.groupby(['Filer','CUSIP', 'SYMBOL', 'Name', 'Entity Type', 'Reporting Period', 'Reporting Quarter', 'Source Date'], as_index=False)[['Shares Held', 'Total Market Value']].sum()\n",
        "      \n",
        "      # identify New or Sold All CUSIPS\n",
        "      result = pd.merge(dfn, dfn1, on=\"CUSIP\", how=\"outer\", suffixes=('_Before', '_Current'), indicator=True)\n",
        "      result['Change Status'] = ''\n",
        "\n",
        "      result.loc[result['_merge'] == 'right_only', [\"Change Status\"]] = 'New'\n",
        "      result.loc[result['_merge'] == 'left_only', [\"Change Status\"]] = 'Sold All'\n",
        "\n",
        "\n",
        "      # Extract New, Sold All\n",
        "      df_lo = result[result['_merge'] == 'left_only']\n",
        "      df_ro = result[result['_merge'] == 'right_only']\n",
        "\n",
        "      # Handle Common CUSIPs\n",
        "      df_bo = pd.merge(dfn, dfn1, on=\"CUSIP\", how=\"inner\", suffixes=('_Before', '_Current'), indicator=True)\n",
        "\n",
        "      df_bo['Change Status'] = np.where(df_bo['Shares Held_Before']< df_bo['Shares Held_Current'], \"Added\",\\\n",
        "                                        np.where(df_bo['Shares Held_Before']>df_bo['Shares Held_Current'], \"Reduced\", \\\n",
        "                                        np.where(df_bo['Shares Held_Before']==df_bo['Shares Held_Current'], \"No Change\",\\\n",
        "                                                \"NA\" )))\n",
        "\n",
        "      # calculate changes in shares for for both\n",
        "      df_bo[\"Changes in Shares Held\"] = df_bo[\"Shares Held_Current\"] - df_bo[\"Shares Held_Before\"]\n",
        "      df_bo[\"Changes in Market Value\"] = df_bo[\"Total Market Value_Current\"] - df_bo[\"Total Market Value_Before\"]\n",
        "\n",
        "      # Set changes to new buys\n",
        "      df_ro['Changes in Shares Held'] = df_ro['Shares Held_Current']\n",
        "      df_ro['Changes in Market Value'] = df_ro['Total Market Value_Current']\n",
        "\n",
        "      # Set changes to Sells\n",
        "      df_lo['Changes in Shares Held'] = df_lo['Shares Held_Before'] * -1\n",
        "      df_lo['Changes in Market Value'] = df_lo['Total Market Value_Before'] * -1\n",
        "\n",
        "      # drop and rename columns\n",
        "      df_lo.drop([col for col in df_lo.columns if 'Current' in col],axis=1, inplace=True)\n",
        "      df_lo.columns = df_lo.columns.str.replace(r'_Before$', '')\n",
        "      df_lo.drop(columns=['_merge'], inplace = True)\n",
        "      df_ro.drop([col for col in df_ro.columns if 'Before' in col],axis=1, inplace=True)\n",
        "      df_ro.columns = df_ro.columns.str.replace(r'_Current$', '')\n",
        "      df_ro.drop(columns=['_merge'], inplace = True)\n",
        "      df_bo.drop([col for col in df_bo.columns if 'Before' in col],axis=1, inplace=True)\n",
        "      df_bo.columns = df_bo.columns.str.replace(r'_Current$', '')\n",
        "      df_bo.drop(columns=['_merge'], inplace = True)\n",
        "\n",
        "      # concat all the dfs\n",
        "\n",
        "      updated_df = pd.concat([df_lo, df_ro, df_bo])\n",
        "\n",
        "\n",
        "      # Calculate Derived Metrics\n",
        "      updated_df['Average Price'] = updated_df['Total Market Value'] / updated_df['Shares Held']\n",
        "      updated_df['Percent of Portfolio'] = updated_df['Total Market Value'] / updated_df['Total Market Value'].sum()\n",
        "      updated_df['Rank'] = updated_df['Total Market Value'].rank(method='max', ascending=False).astype(int)\n",
        "\n",
        "\n",
        "      updated_df.to_csv(p+fileName_n1, index=False)\n",
        "      \n",
        "      # Update process status\n",
        "\n",
        "      #download_status_df.loc[download_status_df[\"File Name\"] == fileName_n1, \"Process Status\"] = \"Latest File\"\n",
        "      #download_status_df.loc[download_status_df[\"File Name\"] == fileName_n, \"Process Status\"] = \"QoQ Processed\"\n",
        "      updateFileStatus(fileName_n1, \"Latest File\")\n",
        "      updateFileStatus(fileName_n, \"QoQ Processed\")\n",
        "      #download_status_df.to_csv(path_downloaded+'download_status.csv', index=False)\n",
        "      print('Completed ' + fileName_n1)\n",
        "      return \"Success\"\n",
        "    else: \n",
        "      print(\"Skipping files: \", fileName_n, fileName_n1, file1EmptyCheck, file2EmptyCheck)\n",
        "      if file2EmptyCheck != 1:\n",
        "        #download_status_df.loc[download_status_df[\"File Name\"] == fileName_n1, \"Process Status\"] = \"Empty File\"\n",
        "        updateFileStatus(fileName_n1, \"Empty File\")\n",
        "      else:\n",
        "        # add required columns\n",
        "        dfn = pd.read_csv(p2+fileName_n1)\n",
        "        cols = ['Title of Class','Market Value', 'Put Call', 'Investment Discretion', 'Sole', 'Shared', 'Source Quarter']\n",
        "        dfn.drop(columns=cols, axis=1, inplace=True)\n",
        "        dfn['Changes in Shares Held'] = None\n",
        "        dfn['Changes in Market Value'] = None\n",
        "        dfn['Change Status'] = \"NA\"\n",
        "        dfn.to_csv(p+fileName_n1, index=False) \n",
        "        #download_status_df.loc[download_status_df[\"File Name\"] == fileName_n1, \"Process Status\"] = \"Latest File\"\n",
        "        updateFileStatus(fileName_n1, \"Latest File\")\n",
        "      if file1EmptyCheck != 1:\n",
        "        \n",
        "        # add required columns\n",
        "        dfn = pd.read_csv(p1+fileName_n)\n",
        "        dfn.drop(columns=cols, axis=1, inplace=True)\n",
        "        dfn['Changes in Shares Held'] = None\n",
        "        dfn['Changes in Market Value'] = None\n",
        "        dfn['Change Status'] = None\n",
        "        dfn['Average Price'] = None\n",
        "        dfn['Percent of Portfolio'] = None\n",
        "        dfn['Rank'] = 0\n",
        "        dfn.to_csv(p+fileName_n, index=False)\n",
        "        #download_status_df.loc[download_status_df[\"File Name\"] == fileName_n, \"Process Status\"] = \"Empty File\"\n",
        "        updateFileStatus(fileName_n, \"Empty File\")\n",
        "      else: \n",
        "        #download_status_df.loc[download_status_df[\"File Name\"] == fileName_n, \"Process Status\"] = \"QoQ Processed\"\n",
        "        updateFileStatus(fileName_n, \"QoQ Processed\")  \n",
        "      #download_status_df.to_csv(path_downloaded+'download_status.csv', index=False)\n",
        "      return \"Error\"\n",
        "  except Exception as e:\n",
        "    se = str(e)\n",
        "    print(\"ERROR in processing file: \" + fileName_n1 + \"Error:\" + se)\n",
        "    # Update process status\n",
        "    #download_status_df.loc[download_status_df[\"File Name\"] == fileName_n1, \"Process Status\"] = \"Error in QoQ\"\n",
        "    updateFileStatus(fileName_n1, \"Error in QoQ\")\n",
        "    #download_status_df.to_csv(path_downloaded+'download_status.csv', index=False)\n",
        "    return \"Error\"\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-pEAtPU5-uZ"
      },
      "source": [
        "def initialFile(fileName):\n",
        "  p = '/content/drive/My Drive/Analytics/13fdata/post1data/'\n",
        "  p1 ='/content/drive/My Drive/Analytics/13fdata/Q420finaldata/'\n",
        "  try:\n",
        "    #file1EmptyCheck = check_empty(fileName) # remove this check\n",
        "    file1EmptyCheck = 1\n",
        "    if file1EmptyCheck == 1:\n",
        "      df = pd.read_csv(p+fileName)\n",
        "      dfn = df.groupby(['Filer','CUSIP', 'SYMBOL', 'Name', 'Entity Type', 'Reporting Period', 'Reporting Quarter', 'Source Date'], as_index=False)[['Shares Held', 'Total Market Value']].sum()\n",
        "      dfn['Changes in Shares Held'] = None\n",
        "      dfn['Changes in Market Value'] = None\n",
        "      dfn['Change Status'] = 'New'\n",
        "      dfn['Average Price'] = dfn['Total Market Value'] / dfn['Shares Held']\n",
        "      dfn['Percent of Portfolio'] = dfn['Total Market Value'] / dfn['Total Market Value'].sum()\n",
        "      dfn['Rank'] = dfn['Total Market Value'].rank(method='max', ascending=False).astype(int)\n",
        "      \n",
        "      updateFileStatus(fileName, \"Initial File\")\n",
        "      #meta_df.loc[meta_df[\"File Name\"] == fileName, \"Process Status\"] = \"Initial File\"\n",
        "\n",
        "      dfn.to_csv(p1+fileName, index=False)\n",
        "      print(\"Completed Intial File\")\n",
        "    else: \n",
        "      print(\"Empty File. Skipping\")\n",
        "      updateFileStatus(fileName, \"Initial File\")\n",
        "      #meta_df.loc[meta_df[\"File Name\"] == fileName, \"Process Status\"] = \"Initial File\"\n",
        "      #meta_df.to_csv(path_downloaded+\"download_status\", index=False)\n",
        "  except Exception as e:\n",
        "    se = str(e)\n",
        "    print(\"ERROR in processing file: \" + fileName + \"Error:\" + se)\n",
        "    # Update process status\n",
        "    #meta_df.loc[meta_df[\"File Name\"] == fileName, \"Process Status\"] = \"Error in QoQ\"\n",
        "    #meta_df.to_csv(path_downloaded+'download_status.csv', index=False)\n",
        "    updateFileStatus(fileName, \"Error in QoQ\")\n",
        "    return \"Error\"\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws2svwfJF-4D"
      },
      "source": [
        "# Function to merge final data csv files\n",
        "def mergeData(xy):\n",
        "  # accepts the list of files to be processed\n",
        "\n",
        "  files = []\n",
        "  p = '/content/drive/My Drive/Analytics/13fdata/post1data/'\n",
        "  # r=root, d=directories, f = files\n",
        "  '''\n",
        "  for r, d, f in os.walk(p):\n",
        "      for file in f:\n",
        "          if '.csv' in file:\n",
        "              files.append(os.path.join(r, file))'''\n",
        "\n",
        "  all_df = []\n",
        "  for f in xy:\n",
        "    fileCheck = check_empty(f)\n",
        "    if fileCheck == 1:\n",
        "      df = pd.read_csv(p+f, sep=',')\n",
        "      df['file'] = f.split('/')[-1]\n",
        "      all_df.append(df)\n",
        "    else:\n",
        "      print(f, \" is Empty\")\n",
        "\n",
        "  \n",
        "\n",
        "  merged_df = pd.concat(all_df)\n",
        "  print(merged_df.shape)\n",
        "  print(merged_df.columns)\n",
        "  merged_df.to_csv('/content/drive/My Drive/Analytics/13fdata/marts/Q420FilerBase.csv', index=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW1m1jxYXHWI"
      },
      "source": [
        "# function to generate Q420FilerBase Analysis\n",
        "def filerBaseAnalysis():\n",
        "  all_df =[]\n",
        "  p = '/content/drive/My Drive/Analytics/13fdata/post1data/'\n",
        "  files = fileNameExtract() # Base Processed Files Only\n",
        "  for f in tqdm(files):\n",
        "    df = pd.read_csv(p+f, sep=',', usecols=[\"Filer\", \"Name\", \"CUSIP\", \"Market Value\", \"Shares Held\", \"Entity Type\", \"Put Call\", \"Reporting Quarter\", \"Reporting Period\",\\\n",
        "                                            \"Total Market Value\", \"Average Price\", \"Percent of Portfolio\", \"Rank\", \"SYMBOL\"])\n",
        "    #df['file'] = f.split('/')[-1]\n",
        "    all_df.append(df)\n",
        "\n",
        "  merged_df = pd.concat(all_df)\n",
        "  merged_df.to_csv('/content/drive/My Drive/Analytics/13fdata/marts/Q420FilerBase.csv', index=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI4-SpEMafp2",
        "outputId": "df95eea8-55d4-474c-9438-7622ca32a516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579,
          "referenced_widgets": [
            "5512631d80b9428e9e866fabea1392e0",
            "18c4a76bef08406f843706e559ed38bc",
            "c901f411ef00493191a7b1e085989bb8",
            "036d7a7e6c904cf08e0e6cceb8831879",
            "48b53a14577b4905816ac7df5196f4e1",
            "a39f5231cbcc4248a410ef5ccd686927",
            "c6a0b0096217468d8d0b3338d488dcb9",
            "33acd37f9e2e47f489dc195831d38e1b"
          ]
        }
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5512631d80b9428e9e866fabea1392e0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=828.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Ignoring:  TRUST CO OF OKLAHOMA\n",
            "Ignoring:  EXANE DERIVATIVES\n",
            "Ignoring:  DnB Asset Management AS\n",
            "Ignoring:  Hudson Capital Management LLC\n",
            "Ignoring:  CenturyLink Investment Management Co\n",
            "Ignoring:  Paragon Capital Management LLC\n",
            "Ignoring:  Resources Investment Advisors, LLC.\n",
            "Ignoring:  Chandler Asset Management, Inc.\n",
            "Ignoring:  Marotta Asset Management\n",
            "Ignoring:  Westside Investment Management, Inc.\n",
            "Ignoring:  Relyea Zuckerberg Hanson LLC\n",
            "Ignoring:  Patten Group, Inc.\n",
            "Ignoring:  Crumly & Associates Inc.\n",
            "Ignoring:  Summit Financial, LLC\n",
            "\n",
            "Ignoring:  CENTRAL TRUST Co\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-d6eec6dba4b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Add to Google Sheet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Q32020FinalAnalysis_test'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# if not exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0msheet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msheet1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mset_with_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiler_data_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_column_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9aAlE29CB_i"
      },
      "source": [
        "# code for QoQ \n",
        "\n",
        "def qoqInitiate():\n",
        "  '''\n",
        "  1. Get all files marked as Base Processed and Current Quarter and get the filers\n",
        "  2. For each filer, check one and only 1 Base Processed\n",
        "  3. For each filer, get the file Marked Latest File from FinalData\n",
        "  4. Pass these 2 files to QoQ Process\n",
        "  5. Land these files in a Q420 Final Data Folder\n",
        "  '''\n",
        "\n",
        "  db = get_mongo_client()\n",
        "  files = fileNameExtract(\"Base Processed\", \"2020Q3\")\n",
        "\n",
        "  for f in tqdm(files):\n",
        "    last_file_data = db.os13fmeta.find({\"$and\": [{\"Filer\": f[\"Filer\"]},{\"Process Status\": \"Latest File\"}]})\n",
        "    last_file = list(last_file_data)\n",
        "    try:\n",
        "      if len(last_file) > 0:\n",
        "        previous_file = last_file[0][\"File Name\"]\n",
        "        current_file = f[\"File Name\"] \n",
        "        # check if Latest File exists\n",
        "        p = '/content/drive/My Drive/Analytics/13fdata/finaldata/'\n",
        "        df = pd.read_csv(p+previous_file)\n",
        "        processed_df = qoqChange(previous_file, current_file)\n",
        "      else: \n",
        "        current_file = f[\"File Name\"]\n",
        "        # call Initial File\n",
        "        processed_df = initialFile(current_file)\n",
        "    except Exception as e:\n",
        "      se = str(e)\n",
        "      print(se)  "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MCGfcUFo3Oo"
      },
      "source": [
        "#DailyPull()\n",
        "# Check Downloaded Empty Files\n",
        "db = get_mongo_client()\n",
        "#file_records = fileNameExtract(\"Downloaded\", \"2020Q3\")\n",
        "#print(file_records)\n",
        "#files = []\n",
        "#for f in file_records:\n",
        "#  files.append(f[\"File Name\"])\n",
        "#x = Parallel(n_jobs=4, backend='multiprocessing')(delayed(postProcess)(f, \"Q420\") for f in tqdm(files))\n",
        "#qoqInitiate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6laKhWzOULPI"
      },
      "source": [
        "# function to generate Q420FilerBase Analysis\n",
        "def finalFileExtract():\n",
        "  files = []\n",
        "  # get Latest and Initial Files\n",
        "  file1 = fileNameExtract(\"Latest File\", \"2020Q3\") \n",
        "  file2 = fileNameExtract(\"Initial File\", \"2020Q3\")\n",
        "  \n",
        "  for f1 in file1:\n",
        "    files.append(f1[\"File Name\"])\n",
        "  for f2 in file2:\n",
        "    files.append(f2[\"File Name\"])\n",
        "  \n",
        "  return files\n",
        "  \n",
        "def finalAnalysis(f):\n",
        "  p = '/content/drive/My Drive/Analytics/13fdata/Q420finaldata/'\n",
        "  # check total market value and remove from final analysis\n",
        "  df = pd.read_csv(p+f, sep=',')\n",
        "  # check header\n",
        "  file_data = metaExtract(f)\n",
        "  an = file_data[\"Accession Number\"]\n",
        "  TMV, Rows = get_header_info(an)\n",
        "  if TMV > 0 or Rows > 0:\n",
        "    r = int(df[\"Total Market Value\"].sum())/TMV\n",
        "    if r>100:\n",
        "      print(\"Ignoring: \", file_data[\"Filer\"])\n",
        "    else:\n",
        "      return df\n",
        "      #all_df.append(df)\n",
        "  else: \n",
        "    pass # ignore the filer\n",
        "\n",
        "files = finalFileExtract()\n",
        "all_df = Parallel(n_jobs=4, backend='multiprocessing')(delayed(finalAnalysis)(f) for f in tqdm(files))\n",
        "merged_df = pd.concat(all_df)\n",
        "merged_df.to_csv('/content/drive/My Drive/Analytics/13fdata/marts/Q32020FinalAnalysis_test.csv', index=False)\n",
        "# Add to Google Sheet\n",
        "title = 'Q32020FinalAnalysis'\n",
        "gc.create(title)  # if not exist\n",
        "sheet = gc.open(title).sheet1\n",
        "set_with_dataframe(sheet, merged_df, include_index=False, include_column_header=True, resize=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZYFwpsIwZVh"
      },
      "source": [
        "def loadFilerData(filer):\n",
        "  filer_data = []\n",
        "  db = get_mongo_client()\n",
        "  meta_df = pd.read_csv('/content/drive/My Drive/Analytics/13fdata/download_status.csv')\n",
        "  #files = list(meta_df[meta_df[\"Filer\"]== filer]['File Name'].unique())\n",
        "  df = meta_df[meta_df[\"Filer\"]==filer]\n",
        "  filer_data_list = df.to_dict('records')\n",
        "  print(filer_data_list)\n",
        "  #db.os13f.delete_many({\"filer\" : filer})\n",
        "  for fd in filer_data_list:\n",
        "    fileID = str(uuid.uuid4())\n",
        "    fileName = fd[\"File Name\"]\n",
        "    filer = fd[\"Filer\"]\n",
        "    quarter = fd[\"Source Quarter\"]\n",
        "    an = fd[\"Accession Number\"]\n",
        "    rank = fd[\"Qrank\"]\n",
        "    df_mg = pd.read_csv('/content/drive/My Drive/Analytics/13fdata/finaldata/'+ fileName)\n",
        "    filer_data.append(df_mg)\n",
        "    filer_data_dict = {\n",
        "                        'fileID': fileID,\n",
        "                        'filer': filer,\n",
        "                        'fileName': fileName,\n",
        "                        'quarter': quarter, \n",
        "                        'acc number': an,\n",
        "                        'rank': rank,\n",
        "                        'filings': df_mg.to_dict('records')}\n",
        "    #db.os13f.insert_one(filer_data_dict)\n",
        "    \n",
        "  filer_data_file = pd.concat(filer_data)\n",
        "  #filer_data_file.to_csv('/content/drive/My Drive/Analytics/13fdata/marts/'+filer+'_final.csv', index=False)\n",
        "  \n",
        "  # put this in gsheet- root folder only\n",
        "\n",
        "  title = '13f-Single-Filer-Analysis'\n",
        "  gc.create(title)  # if not exist\n",
        "  sheet = gc.open(title).sheet1\n",
        "  set_with_dataframe(sheet, filer_data_file, include_index=False, include_column_header=True, resize=False)\n",
        "  \n",
        "  "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk77ay_7Fg1v"
      },
      "source": [
        "def metadbLoad(md):\n",
        "  db.os13fmeta.insert_one(md)\n",
        "\n",
        "def loadMeta():\n",
        "  #db = get_mongo_client()\n",
        "  meta_df = pd.read_csv('/content/drive/My Drive/Analytics/13fdata/download_status.csv')\n",
        "  meta_df_list = meta_df.to_dict('records') \n",
        "  Parallel(n_jobs=32, backend='multiprocessing')(delayed(metadbLoad)(md) for md in tqdm(meta_df_list))\n",
        "\n",
        "def cusipdbLoad(cusip):\n",
        "  db.os13fcusip.insert_one(cusip)\n",
        "def loadCUSIP():\n",
        "  #db = get_mongo_client()\n",
        "  meta_df = pd.read_csv('/content/drive/My Drive/Analytics/13fdata/CUSIP_LKP.csv', usecols=[\"SYMBOL\", \"CUSIP\"])\n",
        "  meta_df_list = meta_df.to_dict('records')\n",
        "  Parallel(n_jobs=32, backend='multiprocessing')(delayed(cusipdbLoad)(cusip) for cusip in tqdm(meta_df_list))\n",
        "\n",
        "\n",
        "# Accession number check\n",
        "\n",
        "def accession_number_check_mdb(an):\n",
        "  db = get_mongo_client()\n",
        "  acc = db.os13fmeta.find({\"Accession Number\": an})\n",
        "  records = list(acc)\n",
        "  if len(records) == 1:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "# insert cusip records\n",
        "def cusip_mdb(cusip, symbol, sector):\n",
        "  db = get_mongo_client()\n",
        "  cusip_record = {\n",
        "        \"SYMBOL\": symbol,\n",
        "        \"CUSIP\": str(cusip),\n",
        "        \"SECTOR\": sector\n",
        "    }\n",
        "  db.os13fcusip.insert_one(cusip_record)\n",
        "\n",
        "# get cusip records\n",
        "\n",
        "def getcusip_mdb():\n",
        "  db = get_mongo_client()\n",
        "  cusip_data = db.os13fcusip.find({})\n",
        "  cusip_data = list(cusip_data)\n",
        "  return cusip_data\n",
        "\n",
        "# ignoredFilers\n",
        "\n",
        "def ignoredFilers_mdb():\n",
        "  meta_df = pd.read_csv('/content/drive/My Drive/Analytics/13fdata/download_status.csv')\n",
        "  filers = list(meta_df[meta_df[\"Process Status\"].isin([\"DQ Check\", \"Error in QoQ\", \"Empty File\"])][\"Filer\"].unique())\n",
        "  meta_df.loc[meta_df[\"Filer\"].isin(filers),\"Process Status\"] = \"Not Included\" # Updates bad filers\n",
        "  print(\"Total Filers Ignored: \", len(filers))\n",
        "  meta_df.to_csv('/content/drive/My Drive/Analytics/13fdata/download_status.csv', index=False)\n",
        "\n",
        "\n",
        "# get file data\n",
        "def fileNameExtract(status, quarter):\n",
        "  db = get_mongo_client()\n",
        "  files = []\n",
        "  file_data = db.os13fmeta.find({\"$and\": [{\"Process Status\": status},{\"Source Quarter\": quarter}]})\n",
        "  #file_data = db.os13fmeta.find({\"Process Status\": \"Downloaded\"})\n",
        "  file_data = list(file_data)\n",
        "  for fd in file_data:\n",
        "    files.append({\"File Name\": fd[\"File Name\"], \n",
        "                  \"Filer\": fd[\"Filer\"]})\n",
        "  return files\n",
        "\n",
        "\n",
        "def metaExtract(f):\n",
        "  db = get_mongo_client()\n",
        "  file_data = db.os13fmeta.find({\"File Name\": f})\n",
        "  for fd in file_data:\n",
        "    return fd\n",
        "\n",
        "# file status\n",
        "\n",
        "def updateFileStatus(fileName, Status):\n",
        "  db = get_mongo_client()\n",
        "  records = db.os13fmeta.update_one({\n",
        "      \"File Name\": fileName\n",
        "  },  {\n",
        "      \"$set\": {\n",
        "          \"Process Status\": Status\n",
        "      }\n",
        "  })"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB5_8X1pxH1m"
      },
      "source": [
        "def cusipDownload():\n",
        "  import requests\n",
        "  from time import sleep\n",
        "  to_be_found = []\n",
        "  # get data from mongo\n",
        "  db = get_mongo_client()\n",
        "  cusip_data = db.os13fcusip.find({})\n",
        "  cusip_data = list(cusip_data)\n",
        "  for cd in cusip_data:\n",
        "    if len(cd) == 3 and cd[\"SYMBOL\"] != \"Not Found\":\n",
        "      to_be_found.append(cd)\n",
        "  for cd in tqdm(to_be_found):\n",
        "    cusip = cd[\"CUSIP\"]\n",
        "    r = requests.get('https://finnhub.io/api/v1/stock/profile2?cusip='+cusip+'&token=bu728rn48v6rghl7pasg')\n",
        "    if r.status_code == 429:\n",
        "      sleep(1)\n",
        "    else: \n",
        "      records = dict(r.json())\n",
        "      if records:  \n",
        "        sector = records[\"finnhubIndustry\"]\n",
        "      else: \n",
        "        sector = \"Unknown\"\n",
        "      \n",
        "      db.os13fcusip.update_one({\n",
        "        \"CUSIP\": cusip\n",
        "          },  {\n",
        "              \"$set\": {\n",
        "                  \"SECTOR\": sector\n",
        "              }\n",
        "          })\n",
        "      sleep(0.5)\n",
        "  apikey = \"bu728rn48v6rghl7pasg\""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXQBe34BC-Sk",
        "outputId": "caac85b1-c56f-4e25-8860-f24181791eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "222633e10ddc4e6fa0a4b8c43d8d54e4",
            "53e1d3908b2c4428bdef328aeafe9888",
            "832b881ad5d3449e8579448f22f70566",
            "e48dc694c3a2458abb2b9d7739228362",
            "f5426d7f81f5442e826b87034ec4f5f8",
            "468c6f25445e49b28f0e2fc31f44a2a7",
            "4ea0319fc075470cb33e7e5bde490806",
            "8d674cbc01424bd0ae50eac1b675080f"
          ]
        }
      },
      "source": [
        "cusipDownload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "222633e10ddc4e6fa0a4b8c43d8d54e4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1624.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP--hWHzLkXG"
      },
      "source": [
        "# get meta_df from mongo or csv\n",
        "xx = []\n",
        "for f in list(meta_df['Filer'].unique()):\n",
        "  temp_df = meta_df[meta_df['Filer'] == f][['File Name', 'Qrank']]\n",
        "  x = temp_df[(temp_df['Qrank'] <= temp_df['Qrank'].max()) & (temp_df['Qrank'] >=temp_df['Qrank'].max()-18)]['File Name'].tolist() # Defines number of files to be included\n",
        "  xx.append(x)\n",
        "xy = [item for sublist in xx for item in sublist]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}